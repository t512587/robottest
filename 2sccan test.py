import pyrealsense2 as rs
import numpy as np
import cv2
from PIL import Image
from datetime import datetime
import time
import os
from pipeline import ImageRetrievalPipeline
from config import Config
from pymycobot.elephantrobot import ElephantRobot, JogMode
from scipy.spatial.transform import Rotation as R

# ğŸ” 4x4 è®Šæ›çŸ©é™£è½‰æ›èˆ‡åè½‰
def invert_transform(T):
    R = T[:3, :3]
    t = T[:3, 3]
    R_inv = R.T
    t_inv = -R_inv @ t
    T_inv = np.eye(4)
    T_inv[:3, :3] = R_inv
    T_inv[:3, 3] = t_inv
    return T_inv

def transform_point(T, P):
    P_h = np.ones(4)
    P_h[:3] = P
    P_transformed = T @ P_h
    return P_transformed[:3]

# ğŸ“· ç›¸æ©Ÿå…§åƒèˆ‡åº§æ¨™è½‰æ›
def pixel_to_camera_coords(cx, cy, depth_mm, fx, fy, cx_cam, cy_cam):
    X = (cx - cx_cam) * depth_mm / fx
    Y = (cy - cy_cam) * depth_mm / fy
    Z = depth_mm
    return np.array([X, Y, Z])

# MODIFIED: æ–°å¢å‡½æ•¸ï¼Œç²å–å®Œæ•´ç›¸æ©Ÿå…§åƒåŠç•¸è®Šä¿‚æ•¸
def get_camera_intrinsics_and_distortion(frames):
    color_frame = frames.get_color_frame()
    intr = color_frame.profile.as_video_stream_profile().get_intrinsics()

    # å¾ intrinsics ç‰©ä»¶ä¸­æå–ç„¦è·å’Œå…‰å­¸ä¸­å¿ƒ
    fx, fy, ppx, ppy = intr.fx, intr.fy, intr.ppx, intr.ppy

    # å»ºç«‹ç›¸æ©ŸçŸ©é™£ (Camera Matrix)
    camera_matrix = np.array([
        [fx, 0, ppx],
        [0, fy, ppy],
        [0, 0, 1]
    ], dtype=np.float32)

    # ç²å–ç•¸è®Šä¿‚æ•¸ (Distortion Coefficients)
    dist_coeffs = np.array(intr.coeffs, dtype=np.float32)

    # è¿”å›ç›¸æ©ŸçŸ©é™£ã€ç•¸è®Šä¿‚æ•¸ã€ä»¥åŠå–®ç¨çš„ fx, fy, ppx, ppy ä»¥ä¾¿èˆŠç¨‹å¼ç¢¼å…¼å®¹
    return camera_matrix, dist_coeffs, fx, fy, ppx, ppy

def get_median_depth(depth_img, cx, cy, window=5):
    h, w = depth_img.shape
    x1, x2 = max(cx - window // 2, 0), min(cx + window // 2 + 1, w)
    y1, y2 = max(cy - window // 2, 0), min(cy + window // 2 + 1, h)
    patch = depth_img[y1:y2, x1:x2]
    valid = patch[patch > 0]
    if len(valid) == 0:
        return None
    return np.median(valid)

# ğŸ“¦ åˆå§‹åŒ–æª¢ç´¢æ¨¡å‹èˆ‡æ‰‹è‡‚
retrieval_pipeline = ImageRetrievalPipeline(Config())
retrieval_pipeline.build_database()

elephant_client = ElephantRobot("192.168.1.159", 5001)
elephant_client.start_client()

# ğŸ“· åˆå§‹åŒ– RealSense pipeline
config1 = rs.config()
config1.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
config1.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)

pipeline1 = rs.pipeline()
profile = pipeline1.start(config1)
align1 = rs.align(rs.stream.color)

frames = pipeline1.wait_for_frames()
aligned_frames = align1.process(frames)

# MODIFIED: å‘¼å«æ–°çš„å‡½æ•¸ä¾†ç²å–æ‰€æœ‰å…§åƒ
camera_matrix, dist_coeffs, fx, fy, cx_cam, cy_cam = get_camera_intrinsics_and_distortion(aligned_frames)

# MODIFIED: å°å‡ºç›¸æ©Ÿå…§åƒå’Œç•¸è®Šä¿‚æ•¸
print("--- RealSense ç›¸æ©Ÿå…§åƒ ---")
print("ç›¸æ©ŸçŸ©é™£ (Camera Matrix, K):")
print(camera_matrix)
print("\nç•¸è®Šä¿‚æ•¸ (Distortion Coefficients):")
print(dist_coeffs)
print("---------------------------\n")


# ğŸ” ç›¸æ©Ÿåˆ°å¤¾çˆªçš„å›ºå®šè®Šæ›çŸ©é™£ï¼ˆTsai-Lenz æ‰‹çœ¼æ¨™å®šçµæœï¼‰
T_camera_gripper = np.array([
 [ 2.90836209e-01,  7.64953618e-02, -9.53709998e-01,  4.46063561e+02],
 [ 6.43395326e-03,  9.96619855e-01,  8.18991341e-02,  6.18706117e+01],
 [ 9.56751224e-01, -2.99553592e-02,  2.89360973e-01,  3.59376086e+02],
 [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00],
])
T_gripper_camera = invert_transform(T_camera_gripper)

# ğŸ“‚ ç¢ºä¿å„²å­˜è³‡æ–™å¤¾å­˜åœ¨
os.makedirs("output_images", exist_ok=True)

print("ğŸ“· æŒ‰ä¸‹ 's' é€²è¡Œå•†å“åµæ¸¬èˆ‡ä½ç½®è½‰æ›")
print("Q: é›¢é–‹ç¨‹å¼")

try:
    while True:
        frames = pipeline1.wait_for_frames()
        aligned_frames = align1.process(frames)
        color_frame = aligned_frames.get_color_frame()
        depth_frame = aligned_frames.get_depth_frame()

        if not color_frame or not depth_frame:
            continue

        color_image_raw = np.asanyarray(color_frame.get_data())
        depth_image = np.asanyarray(depth_frame.get_data())

        # MODIFIED: ä½¿ç”¨å¯¦éš›çš„ç›¸æ©ŸçŸ©é™£å’Œç•¸è®Šä¿‚æ•¸å°å½©è‰²åœ–åƒé€²è¡Œå»ç•¸è®Š
        # é€™ä¸€æ­¥å¾ˆé‡è¦ï¼Œç¢ºä¿å¾ŒçºŒçš„ç‰©é«”åµæ¸¬å’Œåº§æ¨™è¨ˆç®—æ˜¯åœ¨ç„¡ç•¸è®Šçš„åœ–åƒä¸Šé€²è¡Œçš„
        color_image_undistorted = cv2.undistort(color_image_raw, camera_matrix, dist_coeffs)

        cv2.imshow("RealSense", color_image_undistorted) # é¡¯ç¤ºå»ç•¸è®Šå¾Œçš„åœ–åƒ
        key = cv2.waitKey(1)

        if key == ord('s'):
            print("ğŸ” é–‹å§‹å•†å“åµæ¸¬")
            # MODIFIED: å°ç‰©é«”åµæ¸¬ä½¿ç”¨å»ç•¸è®Šå¾Œçš„åœ–åƒ
            rgb_img = cv2.cvtColor(color_image_undistorted, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(rgb_img)

            filename, objects = retrieval_pipeline.yolo_parser.detect_objects(pil_img, conf_threshold=0.7)

            if not objects:
                print("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°ç‰©ä»¶")
                continue

            predictions = []
            for obj in objects:
                cropped = pil_img.crop((obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax']))
                results = retrieval_pipeline.retriever.retrieve_similar_images(
                    cropped,
                    retrieval_pipeline.feature_db,
                    retrieval_pipeline.name_db,
                    retrieval_pipeline.label_db,
                    topk=retrieval_pipeline.config.TOP_K
                )
                predictions.append(results)

            # MODIFIED: åœ¨å»ç•¸è®Šå¾Œçš„åœ–åƒä¸Šé€²è¡Œæ¨™è¨»å’Œå„²å­˜
            annotated_image = color_image_undistorted.copy()

            for i, obj in enumerate(objects):
                xmin, ymin, xmax, ymax = obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax']
                cx = (xmin + xmax) // 2
                cy = (ymin + ymax) // 2

                top1_result = predictions[i][0]
                label_id = top1_result['label']
                label_name = retrieval_pipeline.config.ID2LABEL.get(label_id, "unknown")

                # ç•«æ¡† + ä¸­å¿ƒé» + label
                cv2.rectangle(annotated_image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
                cv2.circle(annotated_image, (cx, cy), 5, (0, 0, 255), -1)
                cv2.putText(annotated_image, f"{label_name}", (xmin, ymin - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)

                depth_median = get_median_depth(depth_image, cx, cy)
                if depth_median is None:
                    print(f"[WARN] ç¬¬{i+1}å€‹ç›®æ¨™ [{label_name}] æ·±åº¦ç„¡æ•ˆï¼Œè·³é")
                    continue

                # åƒç´ åˆ°ç›¸æ©Ÿåº§æ¨™çš„è½‰æ›ä½¿ç”¨å¾ SDK ç²å–çš„ fx, fy, cx_cam, cy_cam
                cam_coords = pixel_to_camera_coords(cx, cy, depth_median, fx, fy, cx_cam, cy_cam)
                print(f"ğŸ“ ç¬¬{i+1}å€‹ç›®æ¨™ [{label_name}] ç›¸æ©Ÿåº§æ¨™ï¼š", cam_coords)

                current_coords = elephant_client.get_coords()
                xyz = current_coords[:3]
                rpy = current_coords[3:]  # å–®ä½æ˜¯åº¦

                # æ§‹å»º Base âœ Gripper çš„å®Œæ•´ 4x4 è®Šæ›çŸ©é™£
                T_base_gripper = np.eye(4)
                T_base_gripper[:3, 3] = np.array(xyz)

                # åŠ å…¥ RPY è§’åº¦ï¼ˆè½‰ç‚ºæ—‹è½‰çŸ©é™£ï¼‰
                rotation = R.from_euler('xyz', rpy, degrees=True).as_matrix()

                T_base_gripper[:3, :3] = rotation

                # å°‡ç›¸æ©Ÿåº§æ¨™è½‰æ›ç‚ºåŸºåº§åº§æ¨™
                P_base = transform_point(T_base_gripper @ T_gripper_camera, cam_coords)
                print(f"ğŸ¯ ç¬¬{i+1}å€‹ç›®æ¨™ [{label_name}] åŸºåº§åº§æ¨™ï¼š", P_base)


            # å„²å­˜åœ–åƒ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"output_images/detected_{timestamp}.jpg"
            cv2.imwrite(save_path, annotated_image)
            print(f"âœ… å·²å„²å­˜è¦–è¦ºåŒ–åœ–åƒï¼š{save_path}")

        elif key == ord('q'):
            print("ğŸ‘‹ é›¢é–‹ç¨‹å¼")
            break

except KeyboardInterrupt:
    print("ğŸ›‘ ç¨‹å¼ä¸­æ–·")

finally:
    pipeline1.stop()
    cv2.destroyAllWindows()
    # å‡è¨­ä½ ç”¨ T_camera_gripper è½‰æ› gripper çš„å‰æ–¹å‘ (1, 0, 0)
    # çœ‹è½‰å‡ºä¾†æ˜¯ä¸æ˜¯ camera çš„ Z è»¸ (0, 0, 1)
    forward_in_gripper = np.array([1, 0, 0])
    forward_in_camera = T_camera_gripper[:3, :3] @ forward_in_gripper
    print("Gripper å‘å‰æ–¹å‘è½‰åˆ°ç›¸æ©Ÿåº§æ¨™ç³»è®Šæˆï¼š", forward_in_camera)
    gripper_y = np.array([0,1,0])
    gripper_z = np.array([0,0,1])

    cam_y = T_camera_gripper[:3, :3] @ gripper_y
    cam_z = T_camera_gripper[:3, :3] @ gripper_z

    print("Gripper Y è»¸è½‰åˆ°ç›¸æ©Ÿåº§æ¨™ç³»ï¼š", cam_y)
    print("Gripper Z è»¸è½‰åˆ°ç›¸æ©Ÿåº§æ¨™ç³»ï¼š", cam_z) 